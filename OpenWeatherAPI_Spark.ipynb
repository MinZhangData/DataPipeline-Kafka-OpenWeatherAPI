{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "\n",
    "class RealTimeDataExtract:\n",
    "    \"\"\"\n",
    "    Class to fetch real-time weather data using an external API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key, city):\n",
    "        self.api_key = api_key\n",
    "        self.city = city\n",
    "        self.base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "    def fetch_data(self):\n",
    "        params = {\n",
    "            'q': self.city,\n",
    "            'appid': self.api_key,\n",
    "            'units': 'metric',  # Temperature in Celsius\n",
    "        }\n",
    "        response = requests.get(self.base_url, params=params)\n",
    "        data = response.json()\n",
    "        if data.get(\"cod\") != 200:\n",
    "            print(\"Error fetching data from API:\", data.get(\"message\"))\n",
    "            return None\n",
    "        return data\n",
    "\n",
    "\n",
    "class DataTransform:\n",
    "    \"\"\"\n",
    "    Class to transform raw API data into a PySpark DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize Spark Session\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .master(\"local[*]\")\\\n",
    "            .appName(\"RealTimeWeatherPipeline\") \\\n",
    "            .getOrCreate()\n",
    "        print(\"Spark Master:\", self.spark.sparkContext.master)\n",
    "\n",
    "    def transform(self, raw_data):\n",
    "        if raw_data is None:\n",
    "            return None\n",
    "\n",
    "        # Define schema for PySpark DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"timestamp\", StringType(), True),\n",
    "            StructField(\"temperature\", DoubleType(), True),\n",
    "            StructField(\"humidity\", IntegerType(), True),\n",
    "            StructField(\"pressure\", IntegerType(), True),\n",
    "            StructField(\"weather_description\", StringType(), True),\n",
    "        ])\n",
    "\n",
    "        # Extract relevant fields\n",
    "        weather_info = [{\n",
    "            'city': raw_data['name'],\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'temperature': raw_data['main']['temp'],\n",
    "            'humidity': raw_data['main']['humidity'],\n",
    "            'pressure': raw_data['main']['pressure'],\n",
    "            'weather_description': raw_data['weather'][0]['description'],\n",
    "        }]\n",
    "\n",
    "        # Convert to PySpark DataFrame\n",
    "        spark_df = self.spark.createDataFrame(weather_info, schema=schema)\n",
    "        return spark_df\n",
    "\n",
    "\n",
    "class RealTimeDataProcess:\n",
    "    \"\"\"\n",
    "    Class to calculate real-time metrics using PySpark operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate(self, spark_df):\n",
    "        if spark_df is None:\n",
    "            return None\n",
    "\n",
    "        # Example: Calculate average temperature and humidity\n",
    "        metrics_df = spark_df \\\n",
    "            .withColumn(\"current_timestamp\", current_timestamp()) \\\n",
    "            .groupBy(\"city\") \\\n",
    "            .agg(\n",
    "                avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "                avg(\"humidity\").alias(\"avg_humidity\")\n",
    "            )\n",
    "\n",
    "        return metrics_df\n",
    "\n",
    "\n",
    "class DataStorage:\n",
    "    \"\"\"\n",
    "    Class to store PySpark DataFrame results in a storage location.\n",
    "    \"\"\"\n",
    "\n",
    "    def store(self, spark_df, file_path):\n",
    "        if spark_df is None:\n",
    "            print(\"No data to store.\")\n",
    "            return\n",
    "\n",
    "        # Append data to a CSV file\n",
    "        spark_df.write \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .csv(file_path)\n",
    "        print(f\"Data stored successfully at: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline for city: London\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/22 20:54:21 WARN Utils: Your hostname, zhangmins-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.0.0.194 instead (on interface en0)\n",
      "24/12/22 20:54:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/22 20:54:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Master: local[*]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-----------+--------+--------+-------------------+\n",
      "|city  |timestamp          |temperature|humidity|pressure|weather_description|\n",
      "+------+-------------------+-----------+--------+--------+-------------------+\n",
      "|London|2024-12-22 20:54:24|7.18       |68      |1012    |few clouds         |\n",
      "+------+-------------------+-----------+--------+--------+-------------------+\n",
      "\n",
      "Metrics Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+------------+\n",
      "|city  |avg_temperature|avg_humidity|\n",
      "+------+---------------+------------+\n",
      "|London|7.18           |68.0        |\n",
      "+------+---------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data stored successfully at: data/real_time_weatherData_Spark.csv\n",
      "Pipeline run completed successfully for city: London\n",
      "Sleeping for 10 seconds...\n",
      "Running pipeline for city: London\n",
      "Spark Master: local[*]\n",
      "+------+-------------------+-----------+--------+--------+-------------------+\n",
      "|city  |timestamp          |temperature|humidity|pressure|weather_description|\n",
      "+------+-------------------+-----------+--------+--------+-------------------+\n",
      "|London|2024-12-22 20:54:47|7.18       |68      |1012    |few clouds         |\n",
      "+------+-------------------+-----------+--------+--------+-------------------+\n",
      "\n",
      "Metrics Data:\n",
      "+------+---------------+------------+\n",
      "|city  |avg_temperature|avg_humidity|\n",
      "+------+---------------+------------+\n",
      "|London|7.18           |68.0        |\n",
      "+------+---------------+------------+\n",
      "\n",
      "Data stored successfully at: data/real_time_weatherData_Spark.csv\n",
      "Pipeline run completed successfully for city: London\n",
      "Sleeping for 10 seconds...\n",
      "Pipeline stopped manually.\n"
     ]
    }
   ],
   "source": [
    "class RealTimePipeline:\n",
    "    \"\"\"\n",
    "    Main Real-Time Pipeline to orchestrate all components.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key, city, file_path):\n",
    "        self.api_key = api_key\n",
    "        self.city = city\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the full pipeline: ingestion, transformation, metrics, and storage.\n",
    "        \"\"\"\n",
    "        print(f\"Running pipeline for city: {self.city}\")\n",
    "\n",
    "        # Step 1: Real-Time Data Ingestion\n",
    "        ingestion = RealTimeDataExtract(self.api_key, self.city)\n",
    "        raw_data = ingestion.fetch_data()\n",
    "        if raw_data is None:\n",
    "            print(\"No data fetched. Skipping this run.\")\n",
    "            return\n",
    "\n",
    "        # Step 2: Data Transformation\n",
    "        transformation = DataTransform()\n",
    "        transformed_data = transformation.transform(raw_data)\n",
    "        if transformed_data is None:\n",
    "            print(\"Transformation failed. Skipping this run.\")\n",
    "            return\n",
    "\n",
    "        transformed_data.show(truncate=False)\n",
    "\n",
    "        # Step 3: Real-Time Metrics Calculation\n",
    "        metrics_calculation = RealTimeDataProcess()\n",
    "        metrics_data = metrics_calculation.calculate(transformed_data)\n",
    "        if metrics_data is None:\n",
    "            print(\"Metrics calculation failed. Skipping this run.\")\n",
    "            return\n",
    "\n",
    "        print(\"Metrics Data:\")\n",
    "        metrics_data.show(truncate=False)\n",
    "\n",
    "        # Step 4: Data Storage\n",
    "        storage = DataStorage()\n",
    "        storage.store(metrics_data, self.file_path)\n",
    "\n",
    "        print(f\"Pipeline run completed successfully for city: {self.city}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # API Key for OpenWeather\n",
    "    API_KEY = \"OPenWeatherAPIKey\" # replace with your own API key\n",
    "    CITY = 'London'  \n",
    "    # FILE_PATH = 'real_time_weather_data_output'  # Directory to store CSV files\n",
    "    FILE_PATH = 'data/real_time_weatherData_Spark.csv'\n",
    "\n",
    "    # Initialize and run the pipeline every 10 seconds\n",
    "    pipeline = RealTimePipeline(API_KEY, CITY, FILE_PATH)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            pipeline.run()\n",
    "            print(\"Sleeping for 10 seconds...\")\n",
    "            time.sleep(10)  # Fetch new data every 10 seconds\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Pipeline stopped manually.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
